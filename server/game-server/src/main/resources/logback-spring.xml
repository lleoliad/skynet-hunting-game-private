<?xml version="1.0" encoding="UTF-8"?>
<!-- 日志级别从低到高分为TRACE < DEBUG < INFO < WARN < ERROR < FATAL，如果设置为WARN，则低于WARN的信息都不会输出 -->
<!-- scan:当此属性设置为true时，配置文档如果发生改变，将会被重新加载，默认值为true -->
<!-- scanPeriod:设置监测配置文档是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。
                 当scan为true时，此属性生效。默认的时间间隔为1分钟。 -->
<!-- debug:当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。 -->
<configuration  scan="true" scanPeriod="10 seconds">
    <contextName>logback</contextName>

    <springProperty name="skynet.logs.path" scope="context" source="${skynet.home}/logs" defaultValue="./logs"/>

    <springProperty name="service.name" scope="context" source="skynet.service.name" defaultValue="_IS_UNDEFINED"/>

    <springProperty name="service.port" scope="context" source="skynet.service.port" defaultValue="_IS_UNDEFINED"/>

    <!-- name的值是变量的名称，value的值时变量定义的值。通过定义的值会被插入到logger上下文中。定义后，可以使“${}”来使用变量。 -->
    <property name="log.path" value="${skynet.logs.path}/server/${service.name}-${service.port}" />
    <!--<property name="log.path" value="${${service.name}.home}/logs" />-->

    <!-- kafka -->
    <!--<contextName>oauth2-auth-server</contextName>-->
    <!--定义日志文件的存储地址 勿在 LogBack 的配置中使用相对路径-->
    <property name="LOG_HOME" value="${log.path}"/>

    <!--kafka访问host-->
    <springProperty name="LOG_KAFKA_HOST" scope="context" source="skynet.config.kafka.url" defaultValue="localhost:9092"/>

    <!--应用名称-->
    <springProperty name="APP_NAME" scope="context" source="spring.application.name" defaultValue="skyet"/>
    <!--LogStash访问host-->
    <springProperty name="LOG_STASH_HOST" scope="context" source="skynet.config.logstash.host" defaultValue="localhost"/>

    <!--0. 日志格式和颜色渲染 -->
    <!-- 彩色日志依赖的渲染类 -->
    <conversionRule conversionWord="clr" converterClass="org.springframework.boot.logging.logback.ColorConverter" />
    <conversionRule conversionWord="wex" converterClass="org.springframework.boot.logging.logback.WhitespaceThrowableProxyConverter" />
    <conversionRule conversionWord="wEx" converterClass="org.springframework.boot.logging.logback.ExtendedWhitespaceThrowableProxyConverter" />
    <!-- 彩色日志格式 -->
    <property name="CONSOLE_LOG_PATTERN" value="${CONSOLE_LOG_PATTERN:-%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${PID:- }){magenta} %clr(---){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}}"/>

    <!--1. 输出到控制台-->
    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <!--此日志appender是为开发使用，只配置最底级别，控制台输出的日志级别是大于或等于此级别的日志信息-->
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>debug</level>
        </filter>
        <encoder>
            <Pattern>${CONSOLE_LOG_PATTERN}</Pattern>
            <!-- 设置字符集 -->
            <charset>UTF-8</charset>
        </encoder>
    </appender>

    <!--2. 输出到文档-->
    <!-- 2.1 level为 DEBUG 日志，时间滚动输出  -->
    <appender name="DEBUG_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 正在记录的日志文档的路径及文档名 -->
        <file>${log.path}/web_debug.log</file>
        <!--日志文档输出格式-->
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n</pattern>
            <charset>UTF-8</charset> <!-- 设置字符集 -->
        </encoder>
        <!-- 日志记录器的滚动策略，按日期，按大小记录 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <!-- 日志归档 -->
            <fileNamePattern>${log.path}/web-debug-%d{yyyy-MM-dd}.%i.log</fileNamePattern>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>100MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
            <!--日志文档保留天数-->
            <maxHistory>15</maxHistory>
        </rollingPolicy>
        <!-- 此日志文档只记录debug级别的 -->
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>debug</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
    </appender>

    <!-- 2.2 level为 INFO 日志，时间滚动输出  -->
    <appender name="INFO_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 正在记录的日志文档的路径及文档名 -->
        <file>${log.path}/web_info.log</file>
        <!--日志文档输出格式-->
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n</pattern>
            <charset>UTF-8</charset>
        </encoder>
        <!-- 日志记录器的滚动策略，按日期，按大小记录 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <!-- 每天日志归档路径以及格式 -->
            <fileNamePattern>${log.path}/web-info-%d{yyyy-MM-dd}.%i.log</fileNamePattern>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>100MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
            <!--日志文档保留天数-->
            <maxHistory>15</maxHistory>
        </rollingPolicy>
        <!-- 此日志文档只记录info级别的 -->
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>info</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
    </appender>

    <!-- 2.3 level为 WARN 日志，时间滚动输出  -->
    <appender name="WARN_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 正在记录的日志文档的路径及文档名 -->
        <file>${log.path}/web_warn.log</file>
        <!--日志文档输出格式-->
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n</pattern>
            <charset>UTF-8</charset> <!-- 此处设置字符集 -->
        </encoder>
        <!-- 日志记录器的滚动策略，按日期，按大小记录 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/web-warn-%d{yyyy-MM-dd}.%i.log</fileNamePattern>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>100MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
            <!--日志文档保留天数-->
            <maxHistory>15</maxHistory>
        </rollingPolicy>
        <!-- 此日志文档只记录warn级别的 -->
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>warn</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
    </appender>

    <!-- 2.4 level为 ERROR 日志，时间滚动输出  -->
    <appender name="ERROR_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 正在记录的日志文档的路径及文档名 -->
        <file>${log.path}/web_error.log</file>
        <!--日志文档输出格式-->
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n</pattern>
            <charset>UTF-8</charset> <!-- 此处设置字符集 -->
        </encoder>
        <!-- 日志记录器的滚动策略，按日期，按大小记录 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/web-error-%d{yyyy-MM-dd}.%i.log</fileNamePattern>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>100MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
            <!--日志文档保留天数-->
            <maxHistory>15</maxHistory>
        </rollingPolicy>
        <!-- 此日志文档只记录ERROR级别的 -->
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>ERROR</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
    </appender>

    <!--&lt;!&ndash;DEBUG日志输出到LogStash&ndash;&gt;-->
    <!--<appender name="LOG_STASH_DEBUG" class="net.logstash.logback.appender.LogstashTcpSocketAppender">-->
    <!--    <filter class="ch.qos.logback.classic.filter.ThresholdFilter">-->
    <!--        <level>DEBUG</level>-->
    <!--    </filter>-->
    <!--    <destination>${LOG_STASH_HOST}:4560</destination>-->
    <!--    <encoder charset="UTF-8" class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">-->
    <!--        <providers>-->
    <!--            <timestamp>-->
    <!--                <timeZone>Asia/Shanghai</timeZone>-->
    <!--            </timestamp>-->
    <!--            &lt;!&ndash;自定义日志输出格式&ndash;&gt;-->
    <!--            <pattern>-->
    <!--                <pattern>-->
    <!--                    {-->
    <!--                    "project": "skynet",-->
    <!--                    "level": "%level",-->
    <!--                    "service": "${APP_NAME:-}",-->
    <!--                    "pid": "${PID:-}",-->
    <!--                    "thread": "%thread",-->
    <!--                    "class": "%logger",-->
    <!--                    "message": "%message",-->
    <!--                    "stack_trace": "%exception{20}"-->
    <!--                    }-->
    <!--                </pattern>-->
    <!--            </pattern>-->
    <!--        </providers>-->
    <!--    </encoder>-->
    <!--</appender>-->

    <!--&lt;!&ndash;ERROR日志输出到LogStash&ndash;&gt;-->
    <!--<appender name="LOG_STASH_ERROR" class="net.logstash.logback.appender.LogstashTcpSocketAppender">-->
    <!--    <filter class="ch.qos.logback.classic.filter.LevelFilter">-->
    <!--        <level>ERROR</level>-->
    <!--        <onMatch>ACCEPT</onMatch>-->
    <!--        <onMismatch>DENY</onMismatch>-->
    <!--    </filter>-->
    <!--    <destination>${LOG_STASH_HOST}:4561</destination>-->
    <!--    <encoder charset="UTF-8" class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">-->
    <!--        <providers>-->
    <!--            <timestamp>-->
    <!--                <timeZone>Asia/Shanghai</timeZone>-->
    <!--            </timestamp>-->
    <!--            &lt;!&ndash;自定义日志输出格式&ndash;&gt;-->
    <!--            <pattern>-->
    <!--                <pattern>-->
    <!--                    {-->
    <!--                    "project": "skynet",-->
    <!--                    "level": "%level",-->
    <!--                    "service": "${APP_NAME:-}",-->
    <!--                    "pid": "${PID:-}",-->
    <!--                    "thread": "%thread",-->
    <!--                    "class": "%logger",-->
    <!--                    "message": "%message",-->
    <!--                    "stack_trace": "%exception{20}"-->
    <!--                    }-->
    <!--                </pattern>-->
    <!--            </pattern>-->
    <!--        </providers>-->
    <!--    </encoder>-->
    <!--</appender>-->

    <!--&lt;!&ndash;业务日志输出到LogStash&ndash;&gt;-->
    <!--<appender name="LOG_STASH_BUSINESS" class="net.logstash.logback.appender.LogstashTcpSocketAppender">-->
    <!--    <destination>${LOG_STASH_HOST}:4562</destination>-->
    <!--    <encoder charset="UTF-8" class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">-->
    <!--        <providers>-->
    <!--            <timestamp>-->
    <!--                <timeZone>Asia/Shanghai</timeZone>-->
    <!--            </timestamp>-->
    <!--            &lt;!&ndash;自定义日志输出格式&ndash;&gt;-->
    <!--            <pattern>-->
    <!--                <pattern>-->
    <!--                    {-->
    <!--                    "project": "skynet",-->
    <!--                    "level": "%level",-->
    <!--                    "service": "${APP_NAME:-}",-->
    <!--                    "pid": "${PID:-}",-->
    <!--                    "thread": "%thread",-->
    <!--                    "class": "%logger",-->
    <!--                    "message": "%message",-->
    <!--                    "stack_trace": "%exception{20}"-->
    <!--                    }-->
    <!--                </pattern>-->
    <!--            </pattern>-->
    <!--        </providers>-->
    <!--    </encoder>-->
    <!--</appender>-->

    <!--&lt;!&ndash;接口访问记录日志输出到LogStash&ndash;&gt;-->
    <!--<appender name="LOG_STASH_RECORD" class="net.logstash.logback.appender.LogstashTcpSocketAppender">-->
    <!--    <destination>${LOG_STASH_HOST}:4563</destination>-->
    <!--    <encoder charset="UTF-8" class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">-->
    <!--        <providers>-->
    <!--            <timestamp>-->
    <!--                <timeZone>Asia/Shanghai</timeZone>-->
    <!--            </timestamp>-->
    <!--            &lt;!&ndash;自定义日志输出格式&ndash;&gt;-->
    <!--            <pattern>-->
    <!--                <pattern>-->
    <!--                    {-->
    <!--                    "project": "skynet",-->
    <!--                    "level": "%level",-->
    <!--                    "service": "${APP_NAME:-}",-->
    <!--                    "class": "%logger",-->
    <!--                    "message": "%message"-->
    <!--                    }-->
    <!--                </pattern>-->
    <!--            </pattern>-->
    <!--        </providers>-->
    <!--    </encoder>-->
    <!--</appender>-->

    <!--DEBUG日志输出到kafka log文件-->
    <appender name="LOG_KAFKA_DEBUG_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <!--日志文件输出的文件名-->
            <FileNamePattern>${log.path}/skynet-service-debug.log.%d{yyyy-MM-dd}.log</FileNamePattern>
            <!--日志文件保留天数-->
            <MaxHistory>30</MaxHistory>
        </rollingPolicy>
        <encoder>
            <!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符-->
            <pattern>%-12(%d{yyyy-MM-dd HH:mm:ss.SSS}) %contextName [%thread] %-5level %logger{36} - %msg%n</pattern>
            <charset>UTF-8</charset>
        </encoder>
    </appender>

    <!--DEBUG日志输出到kafka-->
    <appender name="LOG_KAFKA_DEBUG" class="com.github.danielwegener.logback.kafka.KafkaAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>DEBUG</level>
            <!--<onMatch>ACCEPT</onMatch>-->
            <!--<onMismatch>DENY</onMismatch>-->
        </filter>
        <!--<logger name="org.apache.kafka" level="debug" />-->
        <!--<encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">-->
        <!--    <pattern>%-12(%d{yyyy-MM-dd HH:mm:ss.SSS}) %contextName [%thread] %-5level %logger{36} - %msg%n</pattern>-->
        <!--</encoder>-->
        <encoder charset="UTF-8" class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
            <providers>
                <!-- <mdc /> -->
                <!-- <context /> -->
                <timestamp>
                    <timeZone>Asia/Shanghai</timeZone>
                </timestamp>
                <pattern>
                    <pattern>
                        {
                        "project": "skynet",
                        "type": "skynet_admin_debug_log",
                        "service": "${APP_NAME:-}",
                        "pid": "${PID:-}",
                        "level": "%level",
                        "trace": "%X{X-B3-TraceId:-}",
                        "span": "%X{X-B3-SpanId:-}",
                        "parent": "%X{X-B3-ParentSpanId:-}",
                        "thread": "%thread",
                        "class": "%logger{40}",
                        "message": "%message",
                        "stack_trace": "%exception{20}"
                        }
                    </pattern>
                </pattern>
            </providers>
        </encoder>
        <topic>skynet-service-debug</topic>
        <!-- 我们不关心如何对日志消息进行分区 -->
        <!-- <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy"/> -->
        <!--<keyingStrategy class="com.github.danielwegener.logback.kafka.keying.HostNameKeyingStrategy" />-->
        <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.ContextNameKeyingStrategy" />

        <!-- 使用异步传递。 日志记录不会阻止应用程序线程 -->
        <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy"/>

        <!-- 每个<producerConfig>转换为常规kafka-client配置（格式：key = value） -->
        <!-- 生产者配置记录在这里：https://kafka.apache.org/documentation.html#newproducerconfigs -->
        <!-- bootstrap.servers是唯一必需的 producerConfig -->
        <producerConfig>bootstrap.servers=${LOG_KAFKA_HOST}</producerConfig>
        <!-- 不用等待代理对批的接收进行打包。  -->
        <producerConfig>acks=0</producerConfig>
        <!-- 等待最多1000毫秒并收集日志消息，然后再批量发送 -->
        <producerConfig>linger.ms=1000</producerConfig>
        <!-- 即使生产者缓冲区运行已满，也不要阻止应用程序而是开始丢弃消息 -->
        <producerConfig>max.block.ms=490</producerConfig>
        <!-- use gzip to compress each batch of log messages. valid values: none, gzip, snappy  -->
        <producerConfig>compression.type=gzip</producerConfig>
        <!-- even if the producer buffer runs full, do not block the application but start to drop messages -->
        <producerConfig>block.on.buffer.full=false</producerConfig>
        <!-- 定义用于标识kafka代理的客户端ID -->
        <!--<producerConfig>client.id=${HOSTNAME}-${CONTEXT_NAME}-logback-relaxed</producerConfig>-->

        <!-- 如果kafka不可用，这是后备appender。 -->
        <appender-ref ref="LOG_KAFKA_DEBUG_FILE"/>
    </appender>

    <!--ERROR日志输出到kafka log文件-->
    <appender name="LOG_KAFKA_ERROR_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <!--日志文件输出的文件名-->
            <FileNamePattern>${log.path}/skynet-service-error.log.%d{yyyy-MM-dd}.log</FileNamePattern>
            <!--日志文件保留天数-->
            <MaxHistory>30</MaxHistory>
        </rollingPolicy>
        <encoder>
            <!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符-->
            <pattern>%-12(%d{yyyy-MM-dd HH:mm:ss.SSS}) %contextName [%thread] %-5level %logger{36} - %msg%n</pattern>
            <charset>UTF-8</charset>
        </encoder>
    </appender>

    <!--ERROR日志输出到kafka-->
    <appender name="LOG_KAFKA_ERROR" class="com.github.danielwegener.logback.kafka.KafkaAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>ERROR</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <!--<logger name="org.apache.kafka" level="error" />-->
        <!--<encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">-->
        <!--    <pattern>%-12(%d{yyyy-MM-dd HH:mm:ss.SSS}) %contextName [%thread] %-5level %logger{36} - %msg%n</pattern>-->
        <!--</encoder>-->
        <encoder charset="UTF-8" class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
            <providers>
                <!-- <mdc /> -->
                <!-- <context /> -->
                <timestamp>
                    <timeZone>Asia/Shanghai</timeZone>
                </timestamp>
                <pattern>
                    <pattern>
                        {
                        "project": "skynet",
                        "type": "skynet_admin_error_log",
                        "service": "${APP_NAME:-}",
                        "pid": "${PID:-}",
                        "level": "%level",
                        "trace": "%X{X-B3-TraceId:-}",
                        "span": "%X{X-B3-SpanId:-}",
                        "parent": "%X{X-B3-ParentSpanId:-}",
                        "thread": "%thread",
                        "class": "%logger{40}",
                        "message": "%message",
                        "stack_trace": "%exception{20}"
                        }
                    </pattern>
                </pattern>
            </providers>
        </encoder>
        <topic>skynet-service-error</topic>
        <!-- 我们不关心如何对日志消息进行分区 -->
        <!-- <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy"/> -->
        <!--<keyingStrategy class="com.github.danielwegener.logback.kafka.keying.HostNameKeyingStrategy" />-->
        <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.ContextNameKeyingStrategy" />

        <!-- 使用异步传递。 日志记录不会阻止应用程序线程 -->
        <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy"/>

        <!-- 每个<producerConfig>转换为常规kafka-client配置（格式：key = value） -->
        <!-- 生产者配置记录在这里：https://kafka.apache.org/documentation.html#newproducerconfigs -->
        <!-- bootstrap.servers是唯一必需的 producerConfig -->
        <producerConfig>bootstrap.servers=${LOG_KAFKA_HOST}</producerConfig>
        <!-- 不用等待代理对批的接收进行打包。  -->
        <producerConfig>acks=0</producerConfig>
        <!-- 等待最多1000毫秒并收集日志消息，然后再批量发送 -->
        <producerConfig>linger.ms=1000</producerConfig>
        <!-- 即使生产者缓冲区运行已满，也不要阻止应用程序而是开始丢弃消息 -->
        <producerConfig>max.block.ms=490</producerConfig>
        <!-- use gzip to compress each batch of log messages. valid values: none, gzip, snappy  -->
        <producerConfig>compression.type=gzip</producerConfig>
        <!-- even if the producer buffer runs full, do not block the application but start to drop messages -->
        <producerConfig>block.on.buffer.full=false</producerConfig>
        <!-- 定义用于标识kafka代理的客户端ID -->
        <!--<producerConfig>client.id=${HOSTNAME}-${CONTEXT_NAME}-logback-relaxed</producerConfig>-->

        <!-- 如果kafka不可用，这是后备appender。 -->
        <appender-ref ref="LOG_KAFKA_ERROR_FILE"/>
    </appender>

    <!--业务日志输出到kafka log文件-->
    <appender name="LOG_KAFKA_BUSINESS_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <!--日志文件输出的文件名-->
            <FileNamePattern>${log.path}/skynet-service-business.log.%d{yyyy-MM-dd}.log</FileNamePattern>
            <!--日志文件保留天数-->
            <MaxHistory>30</MaxHistory>
        </rollingPolicy>
        <encoder>
            <!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符-->
            <pattern>%-12(%d{yyyy-MM-dd HH:mm:ss.SSS}) %contextName [%thread] %-5level %logger{36} - %msg%n</pattern>
            <charset>UTF-8</charset>
        </encoder>
    </appender>

    <!--业务日志输出到kafka-->
    <appender name="LOG_KAFKA_BUSINESS" class="com.github.danielwegener.logback.kafka.KafkaAppender">
        <!--<encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">-->
        <!--    <pattern>%-12(%d{yyyy-MM-dd HH:mm:ss.SSS}) %contextName [%thread] %-5level %logger{36} - %msg%n</pattern>-->
        <!--</encoder>-->
        <encoder charset="UTF-8" class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
            <providers>
                <!-- <mdc /> -->
                <!-- <context /> -->
                <timestamp>
                    <timeZone>Asia/Shanghai</timeZone>
                </timestamp>
                <pattern>
                    <!--<pattern>-->
                    <!--    {-->
                    <!--    "project": "skynet",-->
                    <!--    "type": "skynet_admin_business_log",-->
                    <!--    "service": "${APP_NAME:-}",-->
                    <!--    "pid": "${PID:-}",-->
                    <!--    "level": "%level",-->
                    <!--    "trace": "%X{X-B3-TraceId:-}",-->
                    <!--    "span": "%X{X-B3-SpanId:-}",-->
                    <!--    "parent": "%X{X-B3-ParentSpanId:-}",-->
                    <!--    "thread": "%thread",-->
                    <!--    "class": "%logger{40}",-->
                    <!--    "message": "%message",-->
                    <!--    "stack_trace": "%exception{20}"-->
                    <!--    }-->
                    <!--</pattern>-->
                    <pattern>
                        {
                        "project": "skynet",
                        "service": "${APP_NAME:-}",
                        "class": "%logger{40}",
                        "message": "%message"
                        }
                    </pattern>
                </pattern>
            </providers>
        </encoder>
        <topic>skynet-service-business</topic>
        <!-- 我们不关心如何对日志消息进行分区 -->
        <!-- <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy"/> -->
        <!--<keyingStrategy class="com.github.danielwegener.logback.kafka.keying.HostNameKeyingStrategy" />-->
        <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.ContextNameKeyingStrategy" />

        <!-- 使用异步传递。 日志记录不会阻止应用程序线程 -->
        <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy"/>

        <!-- 每个<producerConfig>转换为常规kafka-client配置（格式：key = value） -->
        <!-- 生产者配置记录在这里：https://kafka.apache.org/documentation.html#newproducerconfigs -->
        <!-- bootstrap.servers是唯一必需的 producerConfig -->
        <producerConfig>bootstrap.servers=${LOG_KAFKA_HOST}</producerConfig>
        <!-- 不用等待代理对批的接收进行打包。  -->
        <producerConfig>acks=0</producerConfig>
        <!-- 等待最多1000毫秒并收集日志消息，然后再批量发送 -->
        <producerConfig>linger.ms=1000</producerConfig>
        <!-- 即使生产者缓冲区运行已满，也不要阻止应用程序而是开始丢弃消息 -->
        <producerConfig>max.block.ms=490</producerConfig>
        <!-- use gzip to compress each batch of log messages. valid values: none, gzip, snappy  -->
        <producerConfig>compression.type=gzip</producerConfig>
        <!-- even if the producer buffer runs full, do not block the application but start to drop messages -->
        <producerConfig>block.on.buffer.full=false</producerConfig>
        <!-- 定义用于标识kafka代理的客户端ID -->
        <!--<producerConfig>client.id=${HOSTNAME}-${CONTEXT_NAME}-logback-relaxed</producerConfig>-->

        <!-- 如果kafka不可用，这是后备appender。 -->
        <appender-ref ref="LOG_KAFKA_BUSINESS_FILE"/>
    </appender>

    <!--接口访问记录日志输出到kafka log文件-->
    <appender name="LOG_KAFKA_RECORD_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <!--日志文件输出的文件名-->
            <FileNamePattern>${log.path}/skynet-service-record.log.%d{yyyy-MM-dd}.log</FileNamePattern>
            <!--日志文件保留天数-->
            <MaxHistory>30</MaxHistory>
        </rollingPolicy>
        <encoder>
            <!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符-->
            <pattern>%-12(%d{yyyy-MM-dd HH:mm:ss.SSS}) %contextName [%thread] %-5level %logger{36} - %msg%n</pattern>
            <charset>UTF-8</charset>
        </encoder>
    </appender>

    <!--接口访问记录日志输出到kafka-->
    <appender name="LOG_KAFKA_RECORD" class="com.github.danielwegener.logback.kafka.KafkaAppender">
        <!--<encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">-->
        <!--    <pattern>%-12(%d{yyyy-MM-dd HH:mm:ss.SSS}) %contextName [%thread] %-5level %logger{36} - %msg%n</pattern>-->
        <!--</encoder>-->
        <encoder charset="UTF-8" class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
            <providers>
                <!--<mdc />-->
                <!--<context />-->
                <timestamp>
                    <timeZone>Asia/Shanghai</timeZone>
                </timestamp>
                <pattern>
                    <!--<pattern>-->
                    <!--    {-->
                    <!--    "project": "skynet",-->
                    <!--    "type": "skynet_admin_record_log",-->
                    <!--    "service": "${APP_NAME:-}",-->
                    <!--    "level": "%level",-->
                    <!--    "trace": "%X{X-B3-TraceId:-}",-->
                    <!--    "span": "%X{X-B3-SpanId:-}",-->
                    <!--    "parent": "%X{X-B3-ParentSpanId:-}",-->
                    <!--    "thread": "%thread",-->
                    <!--    "class": "%logger{40}",-->
                    <!--    "message": "%message",-->
                    <!--    "stack_trace": "%exception{20}"-->
                    <!--    }-->
                    <!--</pattern>-->
                    <pattern>
                        {
                        "project": "skynet",
                        "service": "${APP_NAME:-}",
                        "message": "%message"
                        }
                    </pattern>
                </pattern>
            </providers>
        </encoder>
        <topic>skynet-service-record</topic>
        <!-- 我们不关心如何对日志消息进行分区 -->
        <!--<keyingStrategy class="com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy"/>-->
        <!--<keyingStrategy class="com.github.danielwegener.logback.kafka.keying.HostNameKeyingStrategy" />-->
        <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.ContextNameKeyingStrategy" />

        <!-- 使用异步传递。 日志记录不会阻止应用程序线程 -->
        <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy"/>

        <!-- 每个<producerConfig>转换为常规kafka-client配置（格式：key = value） -->
        <!-- 生产者配置记录在这里：https://kafka.apache.org/documentation.html#newproducerconfigs -->
        <!-- bootstrap.servers是唯一必需的 producerConfig -->
        <producerConfig>bootstrap.servers=${LOG_KAFKA_HOST}</producerConfig>
        <!-- 不用等待代理对批的接收进行打包。  -->
        <producerConfig>acks=0</producerConfig>
        <!-- 等待最多1000毫秒并收集日志消息，然后再批量发送 -->
        <producerConfig>linger.ms=1000</producerConfig>
        <!-- 即使生产者缓冲区运行已满，也不要阻止应用程序而是开始丢弃消息 -->
        <producerConfig>max.block.ms=490</producerConfig>
        <!-- use gzip to compress each batch of log messages. valid values: none, gzip, snappy  -->
        <producerConfig>compression.type=gzip</producerConfig>
        <!-- even if the producer buffer runs full, do not block the application but start to drop messages -->
        <producerConfig>block.on.buffer.full=false</producerConfig>
        <!-- 定义用于标识kafka代理的客户端ID -->
        <!--<producerConfig>client.id=${HOSTNAME}-${CONTEXT_NAME}-logback-relaxed</producerConfig>-->

        <!-- 如果kafka不可用，这是后备appender。 -->
        <appender-ref ref="LOG_KAFKA_RECORD_FILE"/>
    </appender>

    <!--https://segmentfault.com/a/1190000023029283?utm_source=tag-newest-->
    <!--https://github.com/danielwegener/logback-kafka-appender/issues/16-->
    <appender name="LOG_KAFKA_DEBUG_ASYNC" class="ch.qos.logback.classic.AsyncAppender">
        <appender-ref ref="LOG_KAFKA_DEBUG" />
    </appender>

    <appender name="LOG_KAFKA_ERROR_ASYNC" class="ch.qos.logback.classic.AsyncAppender">
        <appender-ref ref="LOG_KAFKA_ERROR" />
    </appender>

    <appender name="LOG_KAFKA_BUSINESS_ASYNC" class="ch.qos.logback.classic.AsyncAppender">
        <appender-ref ref="LOG_KAFKA_BUSINESS" />
    </appender>

    <appender name="LOG_KAFKA_RECORD_ASYNC" class="ch.qos.logback.classic.AsyncAppender">
        <appender-ref ref="LOG_KAFKA_RECORD" />
    </appender>

    <!--控制框架输出日志-->
    <logger name="org.slf4j" level="INFO"/>
    <logger name="springfox" level="INFO"/>
    <logger name="io.swagger" level="INFO"/>
    <logger name="org.springframework" level="INFO"/>
    <logger name="org.hibernate.validator" level="INFO"/>

    <!--
        <logger>用来设置某一个包或者具体的某一个类的日志打印级别、
        以及指定<appender>。<logger>仅有一个name属性，
        一个可选的level和一个可选的addtivity属性。
        name:用来指定受此logger约束的某一个包或者具体的某一个类。
        level:用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF，
              还有一个特俗值INHERITED或者同义词NULL，代表强制执行上级的级别。
              如果未设置此属性，那么当前logger将会继承上级的级别。
        addtivity:是否向上级logger传递打印信息。默认是true。
        <logger name="org.springframework.web" level="info"/>
        <logger name="org.springframework.scheduling.annotation.ScheduledAnnotationBeanPostProcessor" level="INFO"/>
    -->

    <!--
        使用mybatis的时候，sql语句是debug下才会打印，而这里我们只配置了info，所以想要查看sql语句的话，有以下两种操作：
        第一种把<root level="info">改成<root level="DEBUG">这样就会打印sql，不过这样日志那边会出现很多其他消息
        第二种就是单独给dao下目录配置debug模式，代码如下，这样配置sql语句会打印，其他还是正常info级别：
        【logging.level.org.mybatis=debug logging.level.dao=debug】
     -->

    <!--
        root节点是必选节点，用来指定最基础的日志输出级别，只有一个level属性
        level:用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF，
        不能设置为INHERITED或者同义词NULL。默认是DEBUG
        可以包含零个或多个元素，标识这个appender将会添加到这个logger。
    -->

    <!-- 4. 最终的策略 -->
    <!-- 4.1 开发环境:打印控制台-->
    <springProfile name="dev">
        <logger name="com.sdcm.pmp" level="debug"/>
    </springProfile>

    <root level="info">
        <appender-ref ref="CONSOLE" />
        <appender-ref ref="DEBUG_FILE" />
        <appender-ref ref="INFO_FILE" />
        <appender-ref ref="WARN_FILE" />
        <appender-ref ref="ERROR_FILE" />
        <!--<appender-ref ref="LOG_STASH_DEBUG"/>-->
        <!--<appender-ref ref="LOG_STASH_ERROR"/>-->
        <!--<appender-ref ref="LOG_KAFKA_DEBUG_ASYNC" />-->
        <!--<appender-ref ref="LOG_KAFKA_ERROR_ASYNC" />-->
    </root>

    <!-- 4.2 生产环境:输出到文档
    <springProfile name="pro">
        <root level="info">
            <appender-ref ref="CONSOLE" />
            <appender-ref ref="DEBUG_FILE" />
            <appender-ref ref="INFO_FILE" />
            <appender-ref ref="ERROR_FILE" />
            <appender-ref ref="WARN_FILE" />
        </root>
    </springProfile> -->

    <!--<logger name="org.skynet.service.base.config.WebLogAspect" level="DEBUG">-->
    <!--    <appender-ref ref="LOG_STASH_RECORD"/>-->
    <!--</logger>-->

    <!--<logger name="org.skynet" level="DEBUG">-->
    <!--    <appender-ref ref="LOG_STASH_BUSINESS"/>-->
    <!--</logger>-->

    <logger name="org.skynet.commons.context.config.WebLogAspect" level="DEBUG">
        <appender-ref ref="LOG_KAFKA_RECORD_ASYNC"/>
    </logger>
    <!--<logger name="org.skynet.admin" level="DEBUG">-->
    <!--    <appender-ref ref="LOG_KAFKA_BUSINESS_ASYNC"/>-->
    <!--</logger>-->

</configuration>